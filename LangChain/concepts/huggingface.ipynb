{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054ea73d",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/langchain\n",
    "\n",
    "Hugging Face and LangChain have collaborated to create a new package called langchain_huggingface. This integration allows users to work with Hugging Face models through LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_token=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f2fe1",
   "metadata": {},
   "source": [
    "pip install langchain_huggingface huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b285520",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "**how to access huggingface models with API**\n",
    "\n",
    "Remote Inference: There are also two ways to use this class. you can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3786753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# provide huggingface model name \n",
    "# repo_id='mistralai/Mistral-7B-Instruct-v0.3'\n",
    "# repo_id='google/gemma-2-9b'\n",
    "repo_id='meta-llama/Llama-3.2-11B-Vision-Instruct'\n",
    "\n",
    "# establishing connection using HuggingFaceEndpoint\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=HF_token)\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614848ce",
   "metadata": {},
   "source": [
    "getting error in invoking the model because the model youâ€™re trying to call does not exist at that endpoint. In other words, is not a valid hosted model on the inference API\n",
    "\n",
    "To find out the models which are supported, go to https://huggingface.co/models and apply filter \"HF Inference API\", after model you will get try that should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbaa9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"what is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152041e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template=\"\"\"\n",
    "Question:\"{question}\n",
    "Answer: Lets think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(template=template,input_variables=['question'])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke('what is gen ai?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11641d82",
   "metadata": {},
   "source": [
    "### HuggingFaceEmbeddings \n",
    "Test-embeddings: utilizes models to convert text into numerical vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a1a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "vectorstore=embeddings.embed_query(\"this is an example sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38883688",
   "metadata": {},
   "source": [
    "### ChatHuggingFace\n",
    "facilitates the integration of chat models from huggingface into langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2888b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "# in huggingface, we can go and create our own endpoint url for a model (paid)\n",
    "llm=HuggingFaceEndpoint(enpoint_url='put your created endpoint url',task='text-generation',max_new_tokens=1024,do_sample=False)\n",
    "chat_model=ChatHuggingFace(llm=llm)\n",
    "chat_model.invoke(\"hugging face is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ed10c",
   "metadata": {},
   "source": [
    "### HuggingFacePipeline\n",
    "Local Inference: Utilizes the HuggingfacePipeline class to run models locally. This approach is suitable when you have sufficient computational resources and seek low-latency responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a35e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe=pipeline('text-generation',model=model,tokenizer=tokenizer, max_new_token=10)\n",
    "llm=HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda715bd",
   "metadata": {},
   "source": [
    "### HuggingFace Tools Integration\n",
    "Incorporates various tools from huggingface, such a text-to-speech or image capturing models, into langchain workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.audio import HuggingFaceTextToSpeechModelInference\n",
    "\n",
    "tts=HuggingFaceTextToSpeechModelInference()\n",
    "audio_output=tts.run(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db78c09",
   "metadata": {},
   "source": [
    "end-to-end project to summarize the youtube or website url, doing modification in project/summarize.Text/py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
